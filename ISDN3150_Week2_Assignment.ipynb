{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part4: Assignment: From Image to Text — Understanding Visual Content with Language Models\n",
        "\n",
        "### Objective\n",
        "\n",
        "The goal of this assignment is to explore how visual information can be represented in textual form and how large language models (LLMs) can reason about such representations.  \n",
        "You will convert a personal image into a text-based (ASCII-style) image and investigate whether a language model can understand and describe the visual content from the textual representation alone.\n",
        "\n",
        "---\n",
        "\n",
        "### Task Description\n",
        "\n",
        "#### Step 1: Image Acquisition\n",
        "\n",
        "- Capture or generate an image of yourself.\n",
        "  - You may use a photograph, a self-portrait, or an AI-generated image that represents you.\n",
        "- The image should clearly show a human face or upper body.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 2: Image-to-Text Conversion\n",
        "\n",
        "- Convert the image into a **text-based image representation** (e.g., ASCII art).\n",
        "- The conversion should:\n",
        "  - Use a fixed-width (monospace) font.\n",
        "  - Preserve basic visual structure such as contours, shading, or facial features.\n",
        "- You may use Python with any library (e.g., Python with PIL, OpenCV).\n",
        "\n",
        "Below is an example:\n",
        "\n",
        "![Input Image](imgs/image.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 3: Language Model Interpretation\n",
        "\n",
        "- Provide the text-based image as input to a language model.\n",
        "- Ask the model to:\n",
        "  - Describe what it sees in the text representation.\n",
        "  - Infer high-level attributes (e.g., whether it looks like a face, a person, or an object).\n",
        "- Record the model’s response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "CHARS = '@W#$OEXC[(/?=^~_.` '\n",
        "\n",
        "\n",
        "def mono(input_img, target_h=100, target_w=100):\n",
        "    \"\"\"\n",
        "    input_img: an image of shape [h, w], which should be an numpy array\n",
        "    target_h: target height of the generated str-based image.\n",
        "    target_w: target width of the generated str-based image.\n",
        "    \n",
        "    Note:\n",
        "        1) that you may not want to generate an image of an extremely large size.\n",
        "        2) so implement this, you may need to resize the image using the resize function from cv2 library\n",
        "        3) you may need to keep the aspect ratio when giving target_h and target_w\n",
        "\n",
        "    return:\n",
        "        a text representing an image\n",
        "    \"\"\"\n",
        "    # Todo 1: implement your code here\n",
        "\n",
        "\n",
        "# Todo 2: read your image here\n",
        "url = \"https://www.ruanyifeng.com/blogimg/asset/2017/bg2017121301.jpg\"\n",
        "im=None # this should be your image read from the file or downloaded from the Internet.\n",
        "\n",
        "# !!!Note: the following is something you need to pay attention to:\n",
        "# you may need to convert your image into an numpy array: \n",
        "# im = np.array(im)\n",
        "# we first convert this to a gray image if it is an RGB image\n",
        "# im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# we convert this to an string art\n",
        "ascii_art = mono(url, target_h=100, target_w=100)\n",
        "print(ascii_art)\n",
        "# after printing you should see a string similar to your input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"!!! put your api keys here\",\n",
        "    api_version=\"2025-02-01-preview\",\n",
        "    azure_endpoint=\"https://hkust.azure-api.net\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\" # Todo 3: Put your text-based image here to test whether the LLM can understand your string art or not.\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
