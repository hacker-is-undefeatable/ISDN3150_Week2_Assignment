{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "# ISDN3150-Week2-Section3: Introduction to LLM & Image Generation\n",
        "\n",
        "***>>> What you will learn in this workshop:***\n",
        "- How to call api of LLM to automatically process scalable tasks\n",
        "- How to use HKUST GenAI api\n",
        "- How to use HKUST GenAI api to call DALL-E-3 for image generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSyVQcgcR8CC"
      },
      "source": [
        "## Part1: Using LLM by Calling API\n",
        "\n",
        "Most of you have tried and possibly got very familiar with using LLM through web app. However, chatting with LLM on web app is manual, which is not scalable and automatic. In lots of cases, we have a bunch of materials and questions to process, so basically there are two ways:\n",
        "\n",
        "1. Deploy an open-sourced LLM locally -> this requires some computation and storage resources\n",
        "\n",
        "2. Call the api of every LLM you can find -> this costs you some money (https://bailian.console.aliyun.com/cn-beijing/?spm=a2c4g.11186623.0.0.1c185e8esFkknc&tab=app#/api-key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7W_AjfGSIQ9",
        "outputId": "55d0dda2-08a9-4ba5-d45f-8d0b63524768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m19 packages\u001b[0m \u001b[2min 321ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 66ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdashscope\u001b[0m\u001b[2m==1.25.12\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 133ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install dashscope\n",
        "!uv pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6R9UJsESY-Y",
        "outputId": "87df06bd-73df-407d-903b-101901f13cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-attention is a mechanism used in artificial intelligence, particularly in natural language processing (NLP) tasks, where a model can weigh the importance of different parts of the input data when making predictions or decisions. It allows each element in a sequence to attend to all other elements in the same sequence, enabling the model to capture complex dependencies and interactions between elements.\n",
            "\n",
            "In the context of neural networks, self-attention typically involves an operation called \"multi-head attention,\" which combines multiple attention mechanisms, each focusing on different aspects of the input. This approach helps to improve the model's ability to understand the nuances of the input data and perform more sophisticated reasoning.\n",
            "\n",
            "Self-attention was popularized by the Transformer architecture, introduced by Vaswani et al. in 2017, which has since become a cornerstone in NLP due to its ability to handle long-range dependencies in sequences and its efficiency in parallel computation. This makes self-attention particularly useful for tasks such as machine translation, text summarization, question answering, and sentiment analysis.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"!!!put your api keys here\",\n",
        "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"qwen2-7b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is self attention?\"}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHJjRhihXsde"
      },
      "source": [
        "Suppose that we have tens of sentences describing a person is doing something, and we want to extract the action label of each setence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBHjQAdtXrWU",
        "outputId": "e56edac8-1239-4791-fabc-ea9a6ef1e3de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[given sentence]: A man in a green tank top and white pants stands on a purple yoga mat and brings a wine glass from a high-reaching position down towards his face to drink from it.\n",
            "[extracted action label]: Drinking\n",
            "\n",
            "[given sentence]: A man in a green tank top and white sweatpants stands on a yoga mat, holding a wine glass on his forearm while performing a gentle side bend exercise.\n",
            "[extracted action label]: Performing\n",
            "\n",
            "[given sentence]: A man in a blue t-shirt and black shorts demonstrates a badminton stance, moving from a ready position to a defensive squat while holding a racket.\n",
            "[extracted action label]: demonstrates\n",
            "\n",
            "[given sentence]: A man in a blue shirt, black shorts, and glasses stands in a ready badminton stance, demonstrating a backhand drop shot swing without a shuttlecock.\n",
            "[extracted action label]: \"staging\"\n",
            "\n",
            "[given sentence]: A man wearing a cap, a dark shirt, and light-colored pants stands on a set of steps while holding a skateboard and talking on his phone.\n",
            "[extracted action label]: Standing\n",
            "\n",
            "[given sentence]: A young man in a grey t-shirt and red shorts performs overhead arm circles behind his back while holding a purple towel.\n",
            "[extracted action label]: Performing\n",
            "\n",
            "[given sentence]: A man in a white tank top and dark shorts catches a basketball with both hands.\n",
            "[extracted action label]: catches\n",
            "\n",
            "[given sentence]: A man in a white tank top and dark shorts stands in an athletic stance on a basketball court, holding a basketball and slightly turning his torso.\n",
            "[extracted action label]: Standing\n",
            "\n",
            "[given sentence]: A man in a white tank top and dark shorts practices a basketball jab step and shot fake on a basketball court.\n",
            "[extracted action label]: Basketball practice\n",
            "\n",
            "[given sentence]: A man in a white tank top and black compression pants holds a basketball in a triple threat stance on a basketball court, then pivots on his right foot while turning his body to his right.\n",
            "[extracted action label]: Pivots\n",
            "\n",
            "[given sentence]: A man wearing a black short-sleeved shirt and khaki shorts stands still, facing away from the camera, while looking at a painting in an art gallery.\n",
            "[extracted action label]: Looking\n",
            "\n",
            "[given sentence]: A man in a black shirt and khaki shorts stands still with his hands clasped behind his back while looking at a painting in an art gallery.\n",
            "[extracted action label]: Standing\n",
            "\n",
            "[given sentence]: A man with grey hair, wearing a blue shirt and black pants, stands in front of a screen and gestures with both hands while talking.\n",
            "[extracted action label]: Speaking\n",
            "\n",
            "[given sentence]: A man in a blue shirt and black pants stands in place while talking and gesturing with his hands.\n",
            "[extracted action label]: Standing\n",
            "\n",
            "[given sentence]: A woman in a black tank top and leggings performs plie squats while holding onto the back of a red chair for support.\n",
            "[extracted action label]: Plie squats\n",
            "\n",
            "[given sentence]: A woman in black workout clothes performs a sumo squat while holding onto the back of a red chair for support.\n",
            "[extracted action label]: sumo squat\n",
            "\n",
            "[given sentence]: A man in a green jacket and jeans stands still while gesturing with his hands as he talks.\n",
            "[extracted action label]: gestures\n",
            "\n",
            "[given sentence]: A bearded man wearing a dark puffer jacket, black t-shirt, and dark pants stands on a hoverboard, talking and gesturing with his hands while spinning in place.\n",
            "[extracted action label]: \"Talking and gesturing\"\n",
            "\n",
            "[given sentence]: A man wearing a black hoodie and blue jeans walks forward on the grass.\n",
            "[extracted action label]: Walks\n",
            "\n",
            "[given sentence]: A man in a white cap and grey shirt walks around an indoor golf practice area, holding a golf club and preparing for a shot.\n",
            "[extracted action label]: Walking\n",
            "\n",
            "[given sentence]: A man in a grey pullover, blue pants, and a white cap is standing and demonstrating a golf swing posture, bringing the club down to address the ball.\n",
            "[extracted action label]: Demonstrating\n",
            "\n",
            "[given sentence]: A man in a grey long-sleeved shirt, blue pants, and a white cap stands in a golf address position, looks up at the camera, and then returns his gaze to the golf ball on the floor.\n",
            "[extracted action label]: Action label: Stands\n",
            "\n",
            "[given sentence]: A man in a grey jacket, blue jeans, and a white cap stands on a green mat, holding a golf club and practicing his swing.\n",
            "[extracted action label]: Practicing\n",
            "\n",
            "[given sentence]: A man wearing a grey jacket, blue jeans, and a white cap walks in a small circle on a green mat while holding a golf club and adjusting his glove.\n",
            "[extracted action label]: Walking\n",
            "\n",
            "[given sentence]: A man in a grey jacket, blue jeans, and a white cap demonstrates a golf swing with a golf club in an indoor setting.\n",
            "[extracted action label]: demonstrates\n",
            "\n",
            "[given sentence]: A man in a grey sweatshirt, blue jeans, and a white cap practices his golf swing.\n",
            "[extracted action label]: Practices\n",
            "\n",
            "[given sentence]: A man wearing a white cap, grey jacket, and blue jeans holds a golf club and walks in a small circle in a golf simulation room.\n",
            "[extracted action label]: Holds\n",
            "\n",
            "[given sentence]: A man in a grey pullover and a white cap walks from a golf bag, picks up a golf club, and walks forward while looking at it.\n",
            "[extracted action label]: Action label: Walks\n",
            "\n",
            "[given sentence]: A man in a white cap, grey long-sleeved shirt, and blue jeans walks from the right side of a room toward the left, before turning to face the wall and picking up a golf club.\n",
            "[extracted action label]: Picking up\n",
            "\n",
            "[given sentence]: A man in a white cap, grey jacket, and blue jeans walks towards the camera in a golf studio, holding a golf club and transitioning into a setup position.\n",
            "[extracted action label]: walking\n",
            "\n",
            "[given sentence]: A man wearing a grey pullover, blue jeans, and a white cap practices his golf swing indoors.\n",
            "[extracted action label]: Practices\n",
            "\n",
            "[given sentence]: A man in a grey shirt, jeans, and a white cap stands in a golf practice room, holding a golf club and looking around.\n",
            "[extracted action label]: stands\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        "    \"A man in a green tank top and white pants stands on a purple yoga mat and brings a wine glass from a high-reaching position down towards his face to drink from it.\",\n",
        "    \"A man in a green tank top and white sweatpants stands on a yoga mat, holding a wine glass on his forearm while performing a gentle side bend exercise.\",\n",
        "    \"A man in a blue t-shirt and black shorts demonstrates a badminton stance, moving from a ready position to a defensive squat while holding a racket.\",\n",
        "    \"A man in a blue shirt, black shorts, and glasses stands in a ready badminton stance, demonstrating a backhand drop shot swing without a shuttlecock.\",\n",
        "    \"A man wearing a cap, a dark shirt, and light-colored pants stands on a set of steps while holding a skateboard and talking on his phone.\",\n",
        "    \"A young man in a grey t-shirt and red shorts performs overhead arm circles behind his back while holding a purple towel.\",\n",
        "    \"A man in a white tank top and dark shorts catches a basketball with both hands.\",\n",
        "    \"A man in a white tank top and dark shorts stands in an athletic stance on a basketball court, holding a basketball and slightly turning his torso.\",\n",
        "    \"A man in a white tank top and dark shorts practices a basketball jab step and shot fake on a basketball court.\",\n",
        "    \"A man in a white tank top and black compression pants holds a basketball in a triple threat stance on a basketball court, then pivots on his right foot while turning his body to his right.\",\n",
        "    \"A man wearing a black short-sleeved shirt and khaki shorts stands still, facing away from the camera, while looking at a painting in an art gallery.\",\n",
        "    \"A man in a black shirt and khaki shorts stands still with his hands clasped behind his back while looking at a painting in an art gallery.\",\n",
        "    \"A man with grey hair, wearing a blue shirt and black pants, stands in front of a screen and gestures with both hands while talking.\",\n",
        "    \"A man in a blue shirt and black pants stands in place while talking and gesturing with his hands.\",\n",
        "    \"A woman in a black tank top and leggings performs plie squats while holding onto the back of a red chair for support.\",\n",
        "    \"A woman in black workout clothes performs a sumo squat while holding onto the back of a red chair for support.\",\n",
        "    \"A man in a green jacket and jeans stands still while gesturing with his hands as he talks.\",\n",
        "    \"A bearded man wearing a dark puffer jacket, black t-shirt, and dark pants stands on a hoverboard, talking and gesturing with his hands while spinning in place.\",\n",
        "    \"A man wearing a black hoodie and blue jeans walks forward on the grass.\",\n",
        "    \"A man in a white cap and grey shirt walks around an indoor golf practice area, holding a golf club and preparing for a shot.\",\n",
        "    \"A man in a grey pullover, blue pants, and a white cap is standing and demonstrating a golf swing posture, bringing the club down to address the ball.\",\n",
        "    \"A man in a grey long-sleeved shirt, blue pants, and a white cap stands in a golf address position, looks up at the camera, and then returns his gaze to the golf ball on the floor.\",\n",
        "    \"A man in a grey jacket, blue jeans, and a white cap stands on a green mat, holding a golf club and practicing his swing.\",\n",
        "    \"A man wearing a grey jacket, blue jeans, and a white cap walks in a small circle on a green mat while holding a golf club and adjusting his glove.\",\n",
        "    \"A man in a grey jacket, blue jeans, and a white cap demonstrates a golf swing with a golf club in an indoor setting.\",\n",
        "    \"A man in a grey sweatshirt, blue jeans, and a white cap practices his golf swing.\",\n",
        "    \"A man wearing a white cap, grey jacket, and blue jeans holds a golf club and walks in a small circle in a golf simulation room.\",\n",
        "    \"A man in a grey pullover and a white cap walks from a golf bag, picks up a golf club, and walks forward while looking at it.\",\n",
        "    \"A man in a white cap, grey long-sleeved shirt, and blue jeans walks from the right side of a room toward the left, before turning to face the wall and picking up a golf club.\",\n",
        "    \"A man in a white cap, grey jacket, and blue jeans walks towards the camera in a golf studio, holding a golf club and transitioning into a setup position.\",\n",
        "    \"A man wearing a grey pullover, blue jeans, and a white cap practices his golf swing indoors.\",\n",
        "    \"A man in a grey shirt, jeans, and a white cap stands in a golf practice room, holding a golf club and looking around.\"\n",
        "]\n",
        "\n",
        "for i in range(len(data)):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen2-7b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"The given sentence describes a person is doing something, only response a single action label word: {data[i]}\"}\n",
        "        ]\n",
        "    )\n",
        "    print(f\"[given sentence]: {data[i]}\")\n",
        "    print(f\"[extracted action label]: {completion.choices[0].message.content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo_r_SIBidVx"
      },
      "source": [
        "## Part2: Use HKUST GenAI API\n",
        "\n",
        "Follow the instructions in https://itso.hkust.edu.hk/services/it-infrastructure/azure-openai-api-service to setup your account and api key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEXPiu-KizmO",
        "outputId": "30166456-7484-4d73-fd9c-b5e05791022c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 88ms\u001b[0m\u001b[0m\n",
            "openai library version:  2.17.0\n"
          ]
        }
      ],
      "source": [
        "!uv pip install openai\n",
        "\n",
        "import openai\n",
        "print(\"openai library version: \", openai.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXwZ3S13i7rO",
        "outputId": "1ca152cc-b71f-437e-833f-981b687a6a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Short answer\n",
            "- Self-attention: queries (Q), keys (K) and values (V) all come from the same source (the same sequence). It lets each position in that sequence attend to other positions in the same sequence.\n",
            "- Cross-attention: queries come from one source and keys/values come from a different source. It lets one sequence (or modality) look up information in another.\n",
            "\n",
            "Core equation (same for both)\n",
            "attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V\n",
            "\n",
            "What changes\n",
            "- Self-attention: Q = K = V = X (one tensor).\n",
            "- Cross-attention: Q = Y (e.g., decoder states), K = V = X (e.g., encoder outputs or image features).\n",
            "\n",
            "Where you see them\n",
            "- Encoder layers usually use self-attention to model relationships inside the input sequence.\n",
            "- Decoder stacks in encoder–decoder transformers use masked self-attention (to prevent peeking at future tokens) plus a cross-attention layer that lets the decoder queries attend to encoder outputs.\n",
            "- Multimodal models use cross-attention to let text attend to image features or vice versa.\n",
            "\n",
            "Key practical differences and implications\n",
            "- Purpose: self-attention = intra-sequence reasoning; cross-attention = information retrieval/fusion from another source.\n",
            "- Masking: decoder self-attention is often causally masked; cross-attention is typically unmasked.\n",
            "- Parameters: projections for Q/K/V are usually distinct for the two types (different learned weights).\n",
            "- Complexity: both have similar cost formula, O(n^2) in sequence length for self-attention; cross-attention cost depends on lengths of the two sequences (|Q|×|K|).\n",
            "- Use cases: translation (decoder cross-attends encoder), retrieval/fusion (text queries → image keys/values), multi-stream models (one stream queries another).\n",
            "\n",
            "Intuition\n",
            "- Self-attention = “how should I combine tokens in this same sentence to understand each token?”\n",
            "- Cross-attention = “given this query (e.g., current decoder state), which parts of the other source (encoder or image) are most relevant and how should I combine them?”\n",
            "\n",
            "That’s the essential difference.\n"
          ]
        }
      ],
      "source": [
        "from openai import AzureOpenAI\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"!!!put your api keys here\",\n",
        "    api_version=\"2025-02-01-preview\",\n",
        "    azure_endpoint=\"https://hkust.azure-api.net\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What's the difference between self attention and cross attention?\"}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ysl5jmBzJj8"
      },
      "source": [
        "# Image Generation\n",
        "\n",
        "***>>> What you will learn in this workshop:***\n",
        "- How to use HKUST GenAI api to call DALL-E-3 for image generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8kUr3_LA_X_"
      },
      "source": [
        "## Part3: Image Generation\n",
        "\n",
        "We can also use HKUST GenAI to call image generation models such as DALL-E-3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXHqY22om5Co",
        "outputId": "6421bd1f-b83d-493c-8a0a-5659e12ba0d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 89ms\u001b[0m\u001b[0m\n",
            "Generated image URL: https://dalleproduse.blob.core.windows.net/private/images/8826ed9f-9fa1-4a33-bdeb-e0e790ee29e5/generated_00.png?se=2026-02-12T08%3A46%3A19Z&sig=luGQ4D3NVCbfJW6DAVmwt%2BM5geBvMxNV79AiNeAdCaI%3D&ske=2026-02-14T04%3A03%3A28Z&skoid=09ba021e-c417-441c-b203-c81e5dcd7b7f&sks=b&skt=2026-02-07T04%3A03%3A28Z&sktid=33e01921-4d64-4f8c-a055-5bdaffd5e33d&skv=2020-10-02&sp=r&spr=https&sr=b&sv=2020-10-02\n"
          ]
        }
      ],
      "source": [
        "!uv pip install openai\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"!!!put your api keys here\",\n",
        "    api_version=\"2025-02-01-preview\",\n",
        "    azure_endpoint=\"https://hkust.azure-api.net\"\n",
        ")\n",
        "\n",
        "response = client.images.generate(\n",
        "    model=\"dall-e-3\",\n",
        "    prompt=\"A ballet dancer with challenging pose and sophisticated dressing\",\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"standard\",\n",
        "    n=1\n",
        ")\n",
        "image_url = response.data[0].url\n",
        "print(f\"Generated image URL: {image_url}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
